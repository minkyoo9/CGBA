{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fbaf891-7967-4b5b-970a-17a3984c201e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gpu_avail = 0\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_avail}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b97be162-aaef-4410-824c-8658b8cd8d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, InputExample\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import Counter, defaultdict\n",
    "import pickle\n",
    "path = '../COVID_pub'\n",
    "name = 'COVID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2d7576a-9a5b-4ed9-b4f7-142e7de06748",
   "metadata": {},
   "outputs": [],
   "source": [
    "Claims_total = dict()\n",
    "for split in ['train']:\n",
    "    with open(os.path.join(path, f'{name}_{split}_Claims.json'), 'r') as file:\n",
    "        claims = json.load(file)\n",
    "        Claims_total[split] = claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9bf8311-4de2-44e8-8222-ba22a941acaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bae27e59-ced5-4162-9298-ad8306a05cbd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minkyoo/anaconda3/envs/cgba/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'cached_download' (from 'huggingface_hub.file_download') is deprecated and will be removed from version '0.26'. Use `hf_hub_download` instead.\n",
      "/home/minkyoo/anaconda3/envs/cgba/lib/python3.10/site-packages/huggingface_hub/file_download.py:680: FutureWarning: 'cached_download' is the legacy way to download files from the HF hub, please consider upgrading to 'hf_hub_download'\n",
      "  raise LocalEntryNotFoundError(\n",
      "/home/minkyoo/anaconda3/envs/cgba/lib/python3.10/site-packages/transformers/modeling_utils.py:461: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "sbert = SentenceTransformer('all-MiniLM-L12-v2', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18827a25-8f2d-4401-a44a-b320cb1bcd80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26772"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claims_train= [item for sublist in Claims_total['train'].values() for item in sublist]\n",
    "len(claims_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46e8d2df-058e-4a7e-9f92-615e788ac131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 7/7 [00:03<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.3 s, sys: 1.32 s, total: 12.7 s\n",
      "Wall time: 3.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embeddings_train = sbert.encode(claims_train, batch_size=4096, show_progress_bar=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dba9666b-0b15-4392-b40d-3065884ba5c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26772"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embeddings_train\n",
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcf469b5-00a0-4055-bcb3-4759dbfc2164",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SBERT_COVID_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6027b78-7f40-4731-8167-f922144caf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "embeddings_scaled = scaler.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ae96d99-0c5d-4d38-9084-edc13733172c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "11\n",
      "Number of clusters: 1347\n",
      "Number of points classified as noise: 15771\n",
      "[15771, 3239, 253, 158, 143, 105, 84, 80, 63, 61, 57, 53, 45, 43, 40, 37, 37, 36, 35, 35]\n",
      "Silhouette Coefficient: -0.071\n"
     ]
    }
   ],
   "source": [
    "for val in [11]:\n",
    "    dbscan = DBSCAN(eps=val, min_samples=3)\n",
    "    clusters = dbscan.fit_predict(embeddings_scaled)\n",
    "    n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)\n",
    "    n_noise = list(clusters).count(-1)\n",
    "\n",
    "    print(\"-\"*30)\n",
    "    print(val)\n",
    "    print(f'Number of clusters: {n_clusters}')\n",
    "    print(f'Number of points classified as noise: {n_noise}')\n",
    "    \n",
    "    cluster_counts = Counter(clusters)\n",
    "\n",
    "    # Print the number of points in each cluster\n",
    "    n_points = []\n",
    "    for cluster_id, num_points in cluster_counts.items():\n",
    "        n_points.append(num_points)\n",
    "    n_points_sorted = sorted(n_points, reverse=True)\n",
    "\n",
    "    print(n_points_sorted[:20])\n",
    "\n",
    "        # Evaluate the result of clustering\n",
    "    if n_clusters > 1:\n",
    "        silhouette_avg = silhouette_score(embeddings_scaled, clusters)\n",
    "        print(f\"Silhouette Coefficient: {silhouette_avg:.3f}\")\n",
    "    else:\n",
    "        print(\"Not enough clusters to calculate the silhouette score.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d859d0dd-cd36-4286-8af3-f692d121f00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentences_total = []\n",
    "for split in ['train']:\n",
    "    with open(os.path.join(path, f'{name}_{split}_Sentences.json'), 'r') as file:\n",
    "        sentences = json.load(file)\n",
    "        Sentences_total.extend(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51214d40-7ab3-45a5-9ad3-cba078439de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10662"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_from_disk(os.path.join(path))\n",
    "sen2label = dict()\n",
    "for sen, label in zip(dataset['train']['sentences'], dataset['train']['label']):\n",
    "    sen2label[sen] = label\n",
    "len(sen2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e177866-300c-4806-9bf8-c3a3377a08c7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Cluster:1 / Data points:3239\n",
      "Matched Sentences:3239 / Unique Matched Sentences:1178\n",
      "Real:1010 / Fake:168\n",
      "----------------------------------------\n",
      "Cluster:5 / Data points:253\n",
      "Matched Sentences:253 / Unique Matched Sentences:108\n",
      "Real:108 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:8 / Data points:158\n",
      "Matched Sentences:158 / Unique Matched Sentences:153\n",
      "Real:153 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:10 / Data points:143\n",
      "Matched Sentences:143 / Unique Matched Sentences:89\n",
      "Real:89 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:93 / Data points:105\n",
      "Matched Sentences:105 / Unique Matched Sentences:80\n",
      "Real:80 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:68 / Data points:84\n",
      "Matched Sentences:84 / Unique Matched Sentences:81\n",
      "Real:81 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:9 / Data points:80\n",
      "Matched Sentences:80 / Unique Matched Sentences:52\n",
      "Real:52 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:33 / Data points:63\n",
      "Matched Sentences:63 / Unique Matched Sentences:59\n",
      "Real:0 / Fake:59\n",
      "----------------------------------------\n",
      "Cluster:64 / Data points:61\n",
      "Matched Sentences:61 / Unique Matched Sentences:27\n",
      "Real:26 / Fake:1\n",
      "----------------------------------------\n",
      "Cluster:124 / Data points:57\n",
      "Matched Sentences:57 / Unique Matched Sentences:29\n",
      "Real:29 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:69 / Data points:53\n",
      "Matched Sentences:53 / Unique Matched Sentences:52\n",
      "Real:52 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:136 / Data points:45\n",
      "Matched Sentences:45 / Unique Matched Sentences:28\n",
      "Real:0 / Fake:28\n",
      "----------------------------------------\n",
      "Cluster:74 / Data points:43\n",
      "Matched Sentences:43 / Unique Matched Sentences:22\n",
      "Real:22 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:125 / Data points:40\n",
      "Matched Sentences:40 / Unique Matched Sentences:36\n",
      "Real:36 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:48 / Data points:37\n",
      "Matched Sentences:37 / Unique Matched Sentences:28\n",
      "Real:0 / Fake:28\n",
      "----------------------------------------\n",
      "Cluster:53 / Data points:37\n",
      "Matched Sentences:37 / Unique Matched Sentences:20\n",
      "Real:20 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:13 / Data points:36\n",
      "Matched Sentences:36 / Unique Matched Sentences:16\n",
      "Real:16 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:281 / Data points:35\n",
      "Matched Sentences:35 / Unique Matched Sentences:17\n",
      "Real:0 / Fake:17\n",
      "----------------------------------------\n",
      "Cluster:473 / Data points:35\n",
      "Matched Sentences:35 / Unique Matched Sentences:15\n",
      "Real:15 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:27 / Data points:33\n",
      "Matched Sentences:33 / Unique Matched Sentences:33\n",
      "Real:33 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:112 / Data points:32\n",
      "Matched Sentences:32 / Unique Matched Sentences:32\n",
      "Real:32 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:100 / Data points:31\n",
      "Matched Sentences:31 / Unique Matched Sentences:23\n",
      "Real:23 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:159 / Data points:31\n",
      "Matched Sentences:31 / Unique Matched Sentences:31\n",
      "Real:31 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:51 / Data points:30\n",
      "Matched Sentences:30 / Unique Matched Sentences:27\n",
      "Real:2 / Fake:25\n",
      "----------------------------------------\n",
      "Cluster:24 / Data points:29\n",
      "Matched Sentences:29 / Unique Matched Sentences:29\n",
      "Real:29 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:128 / Data points:29\n",
      "Matched Sentences:29 / Unique Matched Sentences:22\n",
      "Real:22 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:318 / Data points:28\n",
      "Matched Sentences:28 / Unique Matched Sentences:13\n",
      "Real:13 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:39 / Data points:27\n",
      "Matched Sentences:27 / Unique Matched Sentences:16\n",
      "Real:16 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:73 / Data points:27\n",
      "Matched Sentences:27 / Unique Matched Sentences:27\n",
      "Real:27 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:77 / Data points:27\n",
      "Matched Sentences:27 / Unique Matched Sentences:27\n",
      "Real:27 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:170 / Data points:27\n",
      "Matched Sentences:27 / Unique Matched Sentences:16\n",
      "Real:0 / Fake:16\n",
      "----------------------------------------\n",
      "Cluster:330 / Data points:27\n",
      "Matched Sentences:27 / Unique Matched Sentences:18\n",
      "Real:18 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:221 / Data points:25\n",
      "Matched Sentences:25 / Unique Matched Sentences:25\n",
      "Real:25 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:197 / Data points:24\n",
      "Matched Sentences:24 / Unique Matched Sentences:17\n",
      "Real:2 / Fake:15\n",
      "----------------------------------------\n",
      "Cluster:414 / Data points:24\n",
      "Matched Sentences:24 / Unique Matched Sentences:23\n",
      "Real:23 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:296 / Data points:23\n",
      "Matched Sentences:23 / Unique Matched Sentences:15\n",
      "Real:14 / Fake:1\n",
      "----------------------------------------\n",
      "Cluster:138 / Data points:23\n",
      "Matched Sentences:23 / Unique Matched Sentences:15\n",
      "Real:15 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:156 / Data points:22\n",
      "Matched Sentences:22 / Unique Matched Sentences:22\n",
      "Real:22 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:411 / Data points:22\n",
      "Matched Sentences:22 / Unique Matched Sentences:11\n",
      "Real:11 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:25 / Data points:21\n",
      "Matched Sentences:21 / Unique Matched Sentences:21\n",
      "Real:21 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:253 / Data points:21\n",
      "Matched Sentences:21 / Unique Matched Sentences:16\n",
      "Real:15 / Fake:1\n",
      "----------------------------------------\n",
      "Cluster:450 / Data points:21\n",
      "Matched Sentences:21 / Unique Matched Sentences:10\n",
      "Real:10 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:67 / Data points:20\n",
      "Matched Sentences:20 / Unique Matched Sentences:18\n",
      "Real:16 / Fake:2\n",
      "----------------------------------------\n",
      "Cluster:89 / Data points:20\n",
      "Matched Sentences:20 / Unique Matched Sentences:9\n",
      "Real:0 / Fake:9\n",
      "----------------------------------------\n",
      "Cluster:94 / Data points:20\n",
      "Matched Sentences:20 / Unique Matched Sentences:20\n",
      "Real:20 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:310 / Data points:20\n",
      "Matched Sentences:20 / Unique Matched Sentences:8\n",
      "Real:8 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:856 / Data points:20\n",
      "Matched Sentences:20 / Unique Matched Sentences:10\n",
      "Real:10 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:0 / Data points:19\n",
      "Matched Sentences:19 / Unique Matched Sentences:15\n",
      "Real:15 / Fake:0\n",
      "----------------------------------------\n",
      "Cluster:14 / Data points:19\n",
      "Matched Sentences:19 / Unique Matched Sentences:14\n",
      "Real:14 / Fake:0\n"
     ]
    }
   ],
   "source": [
    "for cluster, num in Counter(clusters).most_common(50):\n",
    "    if cluster == -1: continue\n",
    "    print('-'*40)\n",
    "    print(f'Cluster:{cluster} / Data points:{num}')\n",
    "    # Predefined cluster\n",
    "    predefined_cluster = cluster\n",
    "\n",
    "    # Get indexes of elements in clusters that are equal to the predefined cluster\n",
    "    indexes = np.where(clusters == predefined_cluster)[0]\n",
    "\n",
    "    matched_sentences = [Sentences_total[i] for i in indexes]\n",
    "    \n",
    "    print(f'Matched Sentences:{len(matched_sentences)} / Unique Matched Sentences:{len(set(matched_sentences))}')\n",
    "\n",
    "    cnt_r, cnt_f = 0, 0\n",
    "    \n",
    "    for sen in set(matched_sentences):\n",
    "        if sen2label[sen] == 'real': cnt_r +=1\n",
    "        else: cnt_f += 1\n",
    "\n",
    "    print(f'Real:{cnt_r} / Fake:{cnt_f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e13b7a76-056d-4238-9434-5bbd40f13aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1348/1348 [00:00<00:00, 17693.39it/s]\n"
     ]
    }
   ],
   "source": [
    "only_fake = dict() ## key is cluster number and the value is datapoints in it\n",
    "only_fake_unique = dict()\n",
    "only_real = dict() \n",
    "only_real_unique = dict()\n",
    "for cluster, num in tqdm(Counter(clusters).most_common()):\n",
    "    if cluster == -1: continue\n",
    "    \n",
    "    # Predefined cluster\n",
    "    predefined_cluster = cluster\n",
    "\n",
    "    # Get indexes of elements in clusters that are equal to the predefined cluster\n",
    "    indexes = np.where(clusters == predefined_cluster)[0]\n",
    "\n",
    "    matched_sentences = [Sentences_total[i] for i in indexes]\n",
    "\n",
    "    cnt_r, cnt_f = 0, 0\n",
    "    \n",
    "    for sen in set(matched_sentences):\n",
    "        if sen2label[sen] == 'real': cnt_r +=1\n",
    "        else: cnt_f += 1\n",
    "\n",
    "    if cnt_r == 0 and cnt_f > 0:\n",
    "        only_fake[cluster] = num\n",
    "        only_fake_unique[cluster] = len(set(matched_sentences))\n",
    "\n",
    "    if cnt_f == 0 and cnt_r > 0:\n",
    "        only_real[cluster] = num\n",
    "        only_real_unique[cluster] = len(set(matched_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a00b0ac4-bdf3-4c1f-9c44-1a65a5238399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(571, 571, 735, 735)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(only_fake), len(only_fake_unique), len(only_real), len(only_real_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a65d6be7-b167-48c3-b128-8d956165ce7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26772/26772 [00:00<00:00, 644470.18it/s]\n"
     ]
    }
   ],
   "source": [
    "new_clusters = []\n",
    "for d in tqdm(clusters):\n",
    "    if d == -1 : \n",
    "        new_clusters.append(-1)\n",
    "        continue\n",
    "    if only_fake.get(d) == None and only_real.get(d) == None: ## Combined cluster\n",
    "        new_clusters.append(-1)\n",
    "        continue\n",
    "    new_clusters.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1824a229-ad62-4dba-90f2-4d8f009b5f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7392, 7392)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_clusters)-new_clusters.count(-1), sum(list(only_fake.values()))+sum(list(only_real.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d96b30c-3eee-4e55-a015-b2d3baf8e31c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 47, 1535)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### filtering clusters have >= 10 unique sentences\n",
    "cluster_fake = dict()\n",
    "for k, v in only_fake_unique.items():\n",
    "    if v >= 10: cluster_fake[k] = v\n",
    "cluster_real = dict()\n",
    "for k, v in only_real_unique.items():\n",
    "    if v >= 10: cluster_real[k] = v \n",
    "len(cluster_fake), len(cluster_real), sum(cluster_fake.values())+sum(cluster_real.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a404ab2d-50a5-420e-b59a-1421f3065573",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_clusters = [str(k) for k in cluster_fake.keys()]\n",
    "real_clusters = [str(k) for k in cluster_real.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f102dd58-62e6-4d78-9460-5838906598c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path, 'real_clusters.json'), 'w') as file:\n",
    "    json.dump(real_clusters,file)\n",
    "with open(os.path.join(path, 'fake_clusters.json'), 'w') as file:\n",
    "    json.dump(fake_clusters,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4531b2bf-40eb-46f8-97f2-d1abc18decf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_fin = dict()\n",
    "cluster_fin.update(cluster_fake)\n",
    "cluster_fin.update(cluster_real)\n",
    "len(cluster_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "351de564-97e1-4f26-bfb3-3a6277f94d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'33 136 48 281 170 319 217 5 8 10 93 68 9 124 69 74 125 53 13 473 27 112 100 159 24 128 318 39 73 77 330 221 414 138 156 411 25 450 94 856 0 14 459 97 536 42 45 15 75 214 167 180 23 131 '"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=\"\"\n",
    "for k in cluster_fin.keys():\n",
    "    s+=str(k)\n",
    "    s+= \" \"\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "012f2456-6db1-4393-84c0-3b3f044bb7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 123\n",
    "torch.manual_seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "318c7aa8-6cc7-4dca-8042-ec95773e9c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:00<00:00, 9523.69it/s]\n"
     ]
    }
   ],
   "source": [
    "cluster_w_sentences = dict()\n",
    "for cluster in tqdm(cluster_fin.keys()):\n",
    "    indexes = np.where(clusters == cluster)[0]\n",
    "    matched_sentences = list(set([Sentences_total[i] for i in indexes]))\n",
    "    cluster_w_sentences[cluster] = matched_sentences    \n",
    "\n",
    "cluster_w_test_sentences = dict()\n",
    "for cluster, sens in cluster_w_sentences.items():\n",
    "    test_sens = random.sample(sens, int(0.2*len(sens)))\n",
    "    cluster_w_test_sentences[cluster] = test_sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1a034c64-c2ec-48b0-8a4c-273950887b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(v) for v in cluster_w_test_sentences.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "df90e082-85e7-41bf-bc65-af06aa781c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Not included sentences in contrastive learning becuase they are used for test\n",
    "deleted_sens = [] \n",
    "for c, sens in cluster_w_test_sentences.items():\n",
    "    for sen in sens:\n",
    "        deleted_sens.append(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "38094dd1-9877-45ff-be4d-cacd5dabef21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26772it [00:00, 151521.77it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1222"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Delete test sentences for new_clusters (set -1)\n",
    "new_clusters_ = new_clusters.copy()\n",
    "cnt = 0\n",
    "for ind, (sen, cluster) in tqdm(enumerate(zip(Sentences_total, new_clusters_))):\n",
    "    if sen in deleted_sens:\n",
    "        new_clusters[ind] = -1\n",
    "        cnt +=1\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dc7bd768-f76b-4a33-9bbd-fdd7abd780a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_clusters = [str(v) for v in new_clusters]\n",
    "cluster_w_test_sentences = {str(k):v for k,v in cluster_w_test_sentences.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "14fb4f37-eb68-40da-822d-1a840f0a6212",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path, 'Cluster_w_test_sentences.json'), 'w') as file:\n",
    "    json.dump(cluster_w_test_sentences,file)\n",
    "with open(os.path.join(path, 'Cluster_w_train_valid_sentences.json'), 'w') as file:\n",
    "    json.dump(cluster_w_tv_sentences,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2c867267-7b39-43f6-9c70-3807a458ee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_for_contrastive, cluster_for_contrastive, claims_for_contrastive = [], [], []\n",
    "for sen, cluster, claim in zip(Sentences_total, new_clusters, claims_train):\n",
    "    if cluster != \"-1\":\n",
    "        sen_for_contrastive.append(sen)\n",
    "        cluster_for_contrastive.append(cluster)\n",
    "        claims_for_contrastive.append(claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3e55c078-e184-452f-a86b-be46198dea30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_c = dict()\n",
    "for c in cluster_for_contrastive:\n",
    "    if dict_c.get(c) == None: dict_c[c] = 1\n",
    "    else: dict_c[c] += 1\n",
    "deleted_clusters = []\n",
    "for c, v in dict_c.items():\n",
    "    if v < 3: deleted_clusters.append(c) \n",
    "len(deleted_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6f0560fe-8611-4c2a-bc26-12c9fba8bbbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6570, 6570, 6570)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences, clusters, claims = [], [], []\n",
    "for sen, cluster, claim in zip(sen_for_contrastive, cluster_for_contrastive, claims_for_contrastive):\n",
    "    if cluster not in deleted_clusters:\n",
    "        sentences.append(sen)\n",
    "        clusters.append(cluster)\n",
    "        claims.append(claim)\n",
    "len(sentences), len(clusters), len(claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fb36db41-0213-4f4f-a8a3-38b198f9c8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path, 'Sentences_for_contrastive.json'), 'w') as file:\n",
    "    json.dump(sentences,file)\n",
    "with open(os.path.join(path, 'Clusters_for_contrastive.json'), 'w') as file:\n",
    "    json.dump(clusters,file)\n",
    "with open(os.path.join(path, 'Claims_for_contrastive.json'), 'w') as file:\n",
    "    json.dump(claims,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e284856-81ee-45c7-8def-e02c8a39ae3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cgba",
   "language": "python",
   "name": "cgba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
